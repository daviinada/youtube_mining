sex
length(states_names))
length(states_names)
sex <- rep(c('male', 'female', 'female'), 9)
sex
counts
counts <- sample(2:100,length(states_names))
counts
length(states_names)
states_names
states_abrev
counts <- sample(140000:1000000, length(states_names))
sex <- rep(c('male', 'female', 'female'), 9)
states_names <- unique(brazil_data$name)
states_abrev <- unique(brazil_data$`hc-a2`)
counts <- sample(140000:1000000, length(states_names))
sex <- rep(c('male', 'female', 'female'), 9)
states_names
states_abrev
counts
sex
data <- cbind(states_names, states_abrev, counts, sex)
write.table(file='data_text.txt', data, row.names = F, sep = '\t', quote = F)
data
runApp('Desktop/dashboard_alpha/aplha_prototype.R')
install.package('benchmarkme)
install.package('benchmarkme)
d
install.package('benchmarkme')
install.packages('benchmarkme')
library(Rfacebook)
install.packages('Rfacebook')
library(tidyverse)
install.packages('tidyverse')
library(ggalt)
install.packages('readxl')
124.50/115
install.packages('reprex')
install.packages('tidyverse')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
knitr::opts_chunk$set(echo = TRUE)
concat_data$Cabin[which(concat_data$Cabin == '')] <- NA
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mice)
library(dplyr)
library(ggplot2)
library(stringr)
library(purrr)
library(corrplot)
train <- read.csv(file='train.csv', sep=',', stringsAsFactors = FALSE)
train$dataTag <- 'train'
test <-  read.csv(file='test.csv', sep=',', stringsAsFactors = FALSE)
test$dataTag  <- 'test'
test$Survived <- NA
concat_data <- train %>% bind_rows(test)
head(concat_data)
summary(concat_data)
concat_data$Cabin[which(concat_data$Cabin == '')] <- NA
concat_data$Embarked[which(concat_data$Embarked == '')] <- NA
concat_data$Survived <- as.factor(concat_data$Survived)
concat_data$Pclass  <- as.factor(concat_data$Pclass)
concat_data$Sex <- as.factor(concat_data$Sex)
concat_data$Ticket <- as.factor(concat_data$Ticket)
concat_data$Cabin <- as.factor(concat_data$Cabin)
concat_data$Embarked <- as.factor(concat_data$Embarked)
concat_data$dataTag  <- as.factor(concat_data$dataTag)
# Tem um NA tambem, pertence a Pclass 3, vou substituir pela media dessa classe
which(is.na(concat_data$Fare))
concat_data[1044, ]
# Substituindo
concat_data$Fare[1044] <- mean( concat_data$Fare[which(concat_data$Pclass == 3)], na.rm = TRUE)
# Observei 2 NA's em Embarked
concat_data[ which(is.na(concat_data$Embarked)), ]
concat_data$Ticket
# Onde sera que eles embarcaram?
# Por serem classe 1, provavelmente embarcaram em S ou C
table(concat_data$Embarked, concat_data$Pclass)
concat_data$Embarked[ which(is.na(concat_data$Embarked)) ] <- factor("S")
length(concat_data$Name) # n nomes
# preciso tratar melhor, fazer split na virgula e no ponto para capturar o sobrenome e o pronome de tratamento
names <- strsplit(concat_data$Name, ',')
names_no_space <- lapply(names, function(x) gsub(' ', '',x))
# Sobrenpome
surname <- lapply(names_no_space , function(x) x[[1]] )
surname <- unlist(surname)
treat_pron <- lapply(names_no_space , function(x) x[[2]])
treat_pron <- lapply(treat_pron, function(x) strsplit(x, '\\.', perl = TRUE)[[1]])
treat_pron <- map(treat_pron, 1) %>% unlist()
# Adicionando as novas colunas
concat_data$surname <- as.factor(surname)
concat_data$treat_pron <- as.factor(treat_pron)
summary(concat_data)
head(concat_data)
# Vamos ver a distribuicao da idade
# Menores de idade possuem o pronome de tratamento Master
concat_data %>%
select(Age, treat_pron) %>%
na.omit() %>%
ggplot(aes(y=Age, x=treat_pron, fill=treat_pron)) +
geom_boxplot() +
theme_bw()
# Minha primeira estrategia sera usar a idade media de cada classe para preencher os dados faltantes
class_with_na <- concat_data$treat_pron[is.na(concat_data$Age)]
as.character(class_with_na) %>% table()
mean_age_by_class <- concat_data %>%
filter(str_detect(treat_pron, "Dr|Miss|Mr|Mrs|Ms|Master")) %>%
dplyr::group_by(treat_pron) %>%
dplyr::summarize(mean_age = mean(Age, na.rm = TRUE))
mean_age_by_class
for(i in 1:nrow(concat_data)){
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Dr'){
concat_data$Age[i] <- 43.6
}
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Miss'){
concat_data$Age[i] <- 21.8
}
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Mr'){
concat_data$Age[i] <- 32.3
}
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Mrs'){
concat_data$Age[i] <- 37.0
}
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Ms'){
concat_data$Age[i] <- 28.0
}
if(is.na(concat_data$Age[i]) & concat_data$treat_pron[i] == 'Master'){
concat_data$Age[i] <- 5.48
}
}
summary(concat_data)
# Segunda estrategia, fazer regressao da idade usando random forest
# Ordenar por sobrenome
concat_data <- concat_data %>%
arrange(surname)
train_na <- concat_data %>%
select(Age, Pclass, SibSp, Parch, Fare, treat_pron,Sex) %>%
na.omit()
age_model <- train(Age ~ Pclass + Sex  + SibSp + Parch +
Fare + surname + treat_pron ,
data= train_na,
method= 'rf',
tuneGrid= data.frame(mtry= c(2,3,4,5,6,8,10)),
trControl= trainControl(method= 'cv', number= 10,
verboseIter= TRUE))
library(dplyr)
library(tidyr)
mtcars
mtcars %>% contains('cyl')
mtcars %>% filter(contains('cyl'))
library(stringr)
mtcars %>% filter(str_match('cyl'))
mtcars %>% filter(contains('cyl'))
mtcars[ ,cyl]
mtcars[ ,'cyl']
retorna_coluna <- function(df, coluna){
return(df[ ,coluna])
}
retorna_coluna(mtcars, 'cyl')
shiny::runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
runApp('Desktop/teste')
table <- read.csv(file='Desktop/teste_excel.csv')
head(table)
library(readxl)
table <- read_excel(sheet = 'Desktop/teste_excel.xls')
table <- read_excel(sheet = '/home/davi/Desktop/teste_excel.xls')
table <- read_excel(path =  '/home/davi/Desktop/teste_excel.xls')
head(table)
table(table$driver_id)
library(dplyr)
table %>% group_by(driver_id) %>%
summarise(mean = mean())
table %>% group_by(driver_id) %>%
summarise(mean = mean( n() ))
table %>% group_by(driver_id) %>%
summarise(mean = mean( n() )) %>%
arrange(desc(driver_id))
table %>% group_by(driver_id) %>%
summarise(mean = mean( n() )) %>%
arrange(desc(mean))
table <- read_excel(path =  '/home/davi/Desktop/teste_excel.xls')
head(table)
table <- as.data.frame(table)
head(table)
table$driver_id <- as.character(table$driver_id)
head(table)
table %>% group_by(driver_id) %>%
summarise(mean = mean( n() )) %>%
arrange(desc(mean))
table %>% group_by(driver_id) %>%
summarise(mean = sum(n())/ n() ) %>%
arrange(desc(mean))
table(table$driver_id)
table %>% group_by(driver_id) %>%
summarise(mean = sum(n())/ n() ) %>%
arrange(desc(mean))
table %>% group_by(driver_id) %>%
summarise(mean = mean( n() )) %>%
arrange(desc(mean))
table %>% group_by(driver_id) %>%
summarize(mean = mean( n() )) %>%
arrange(desc(mean))
head(table)
table %>% group_by(driver_id) %>%
summarize(mean = sum(`earnings (reais)`)/ n() ) %>%
arrange(desc(mean))
head(table)
head(table)
boxplot(table$`earnings (reais)`)
boxplot(table$`distance (meters)`)
boxplot(table$`duration (minutes)`)
boxplot(table$`duration (minutes)`)
boxplot(table$`earnings (reais)`)
summary(table)
table$driver_id == '11013'
which(table$driver_id == '11013')
table[which(table$driver_id == '11013'), ]
table[which(table$driver_id == '43838'), ]
table[which(table$driver_id == '90731'), ]
table[which(table$driver_id == '11013'), ] %>% summary()
table[which(table$driver_id == '11013'), ] %>% summary()
table[which(table$driver_id == '43838'), ] %>% summary()
table[which(table$driver_id == '90731'), ] %>% summary()
boxplot(table$`earnings (reais)`)
boxplot(table$`distance (meters)`)
boxplot(table$`distance (meters)`)
boxplot(table$`distance (meters)`)
boxplot(table$`duration (minutes)`)
boxplot(table$`earnings (reais)`)
table[which(table$driver_id == '90731'), ] %>% summary()
table[which(table$driver_id == '90731'), ]
table[which(table$driver_id == '11013'), ] %>% summary()
table[which(table$driver_id == '11013'), ] %>% summary()
table[which(table$driver_id == '11013'), ]
librar(ggplot2)
library(ggplot2)
head(table)
mean(table$`distance (meters)`)
mean(table$`distance (meters)`, na.rm = T)
table %>% filter(product == 'Loggi Corp')
table(table$product)
table %>% filter(product == 'Corp')
table %>% filter(product == 'Corp') %>%
summarise(mean(`distance (meters)`, na.rm = TRUE))
head(table)
filter(product == 'Presto') %>%
mean_earn = mean(`earnings (reais)`, na.rm = TRUE)
table %>%
filter(product == 'Presto') %>%
summarise(mean_dist = mean(`distance (meters)`, na.rm = TRUE)/1000,
mean_earn = mean(`earnings (reais)`, na.rm = TRUE)
)
17.85938 / 4.451594
table %>%
group_by(driver_id) %>%
summarise(conts = n()) %>%
arrange(counts)
table %>%
group_by(driver_id) %>%
summarise(counts = n()) %>%
arrange(counts)
table %>%
group_by(driver_id) %>%
summarise(counts = n()) %>%
arrange(desc(counts))
(700*80)/100
(500*60)/100
(800*90)/100
560 + 300 + 720
700 + 500 + 800
1580/2000
10000/450
63*0.5
63*0.05
(63*0.05)/100
63-3.15
59.85*110
5000/100
450*20
450-90
360*30
360*30
500*30
360-108
50*50
20^2
sqrt(800)
15^2
225+400sqrt(800)
225+400
sqrt(62)
sqrt(625)
# Get youtube caption
library(rvest)
library(lexiconPT)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(dygraphs)
library(plotly)
library(tuber)
library(stringr)
library(PTtextmining)
library(tm)
library(wordcloud)
library(RColorBrewer)
install.packages('tuber')
install.packages('tm')
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(tm)
install.packages('tm')
library(tm)
setwd('/home/davi/Desktop/work_butanta/youtube_scraper/')
setwd('/home/davi/Desktop/youtube_mining/')
client_id <- '812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com'
key <- 'SblVJsud6AqXdqKpQJamZRxJ'
yt_oauth(client_id, key)
all_vid_nerdologia <- readRDS('all_vid_nerdologia.RDS')
str(all_vid_nerdologia)
date_split_list <- strsplit(x = as.character(all_vid_nerdologia$publication_date), split = 'T')
date_split_list <- lapply(date_split_list, function(x){ x[1] } ) %>% unlist()
all_vid_nerdologia['publication_date'] <- date_split_list
all_vid_nerdologia$publication_date <- as.Date(all_vid_nerdologia$publication_date)
all_vid_nerdologia$viewCount <-  as.numeric(all_vid_nerdologia$viewCount)
all_vid_nerdologia$likeCount <- as.numeric(all_vid_nerdologia$likeCount)
all_vid_nerdologia$dislikeCount <- as.numeric(all_vid_nerdologia$dislikeCount)
all_vid_nerdologia$favoriteCount <- as.numeric(all_vid_nerdologia$favoriteCount)
all_vid_nerdologia$commentCount <- as.numeric(all_vid_nerdologia$commentCount)
all_vid_nerdologia[which(all_vid_nerdologia$dislikeCount == max(all_vid_nerdologia$dislikeCount)), ]
all_vid_nerdologia[which(all_vid_nerdologia$likeCount == max(all_vid_nerdologia$likeCount)), ]
all_vid_nerdologia$publication_date <- lubridate::as_datetime(all_vid_nerdologia$publication_date)
# Cleanning data title
all_vid_nerdologia$title <- gsub('\\s\\|\\s.*$', '', all_vid_nerdologia$title)
# tabela processada
all_vid_nerdologia_gather <- all_vid_nerdologia %>%
mutate(proportion_like = likeCount/ viewCount,
proportion_dislike = dislikeCount/ viewCount) %>%
gather(key= type_counts, value= counts,
c(likeCount, dislikeCount, commentCount, viewCount))
str(all_vid_nerdologia)
plot_obj <- all_vid_nerdologia_gather %>%
filter(stringr::str_detect('likeCount|dislikeCount|commentCount', type_counts)) %>%
ggplot(aes(x= publication_date, y= as.numeric(counts), col= type_counts)) +
geom_line() +
geom_point() +
scale_x_date() +
theme_bw()
str(all_vid_nerdologia)
# Plotly
all_vid_nerdologia %>%
mutate(proportion_like = likeCount / viewCount,
proportion_dislike = dislikeCount / viewCount) %>%
arrange(publication_date) %>%
plot_ly() %>%
add_trace(x = ~publication_date, y = ~viewCount,
name = 'View count', type = 'scatter',
mode ="markers+lines", text = ~paste('Video: ', title),
line = list(color = '#440154FF ', width = 4)) %>%
add_trace(x = ~publication_date, y = ~likeCount,
name = 'Like count', type = 'scatter',
mode ="markers+lines", text = ~paste('Video: ', title),
line = list(color = '#39568CFF', width = 4)) %>%
add_trace(x = ~publication_date, y = ~dislikeCount,
name = 'Dislike count', type = 'scatter',
mode = "markers+lines", text = ~paste('Video: ', title),
line = list(color = '#29AF7FFF', width = 4)) %>%
add_trace(x = ~publication_date, y = ~commentCount,
name = 'Comment count', type = 'scatter',
mode = "markers+lines", text = ~paste('Video: ', title),
line = list(color = '#FDE725FF', width = 4)) %>%
layout(xaxis = list(title = "Video publication date"),
yaxis = list (title = "Metric's count"),
font =  list(size = 16),
hovermode = 'compare')
html_pages
all_vid_nerdologia_gather
all_vid_nerdologia_gather$id[1:3]
get_url_capition <- function(video_id){
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=', id_vector, '&language=asr')
html_page_text <- lapply(url_request, function(x){ html_text(read_html(x))  })
text_df <- data.frame(doc_id = id_vector, html_page_text, stringsAsFactors = FALSE , drop=FALSE)
return(text_df)
}
get_url_capition(all_vid_nerdologia_gather$id[1:3])
get_url_capition <- function(video_id){
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=', id_vector, '&language=asr')
html_page_text <- lapply(url_request, function(x){ html_text(read_html(x))  })
text_df <- data.frame(doc_id = video_id, html_page_text, stringsAsFactors = FALSE , drop=FALSE)
return(text_df)
}
get_url_capition(all_vid_nerdologia_gather$id[1:3])
get_url_capition <- function(video_id){
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=', id_vector, '&language=asr')
html_page_text <- lapply(url_request, function(x){ html_text(read_html(x))  })
text_df <- data.frame(doc_id = video_id, html_page_text, stringsAsFactors = FALSE , drop=FALSE)
return(text_df)
}
get_url_capition(all_vid_nerdologia_gather$id[1:3])
get_url_capition <- function(video_id){
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=', video_id, '&language=asr')
html_page_text <- lapply(url_request, function(x){ html_text(read_html(x))  })
text_df <- data.frame(doc_id = video_id, html_page_text, stringsAsFactors = FALSE , drop=FALSE)
return(text_df)
}
get_url_capition(all_vid_nerdologia_gather$id[1:3])
get_url_capition <- function(video_id){
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=', video_id, '&language=asr')
html_page_text <- lapply(url_request, function(x){ html_text(read_html(x))  })
text_df <- data.frame(doc_id = video_id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
return(text_df)
}
teste <- get_url_capition(all_vid_nerdologia_gather$id[1:3])
teste
df_corpus <- Corpus(DataframeSource(video_id))
df_corpus <- Corpus(DataframeSource(teste))
teste <- get_url_capition(all_vid_nerdologia_gather$id[2])
teste <- get_url_capition(all_vid_nerdologia_gather$id[86])
teste
df_corpus <- Corpus(DataframeSource(teste))
df_corpus_filtered <- df_corpus %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
inspect(df_corpus_filtered)
inspect(df_corpus)
#Criando a matrix de termos:
df_corpus_tf <- TermDocumentMatrix(df_corpus_filtered, control = list(minWordLength = 4))
df_corpus_df = as.matrix(df_corpus_tf)
?TermDocumentMatrix
v <- sort(rowSums(as.matrix(df_corpus_df)), decreasing=TRUE)
df <- data.frame(word=names(v), freq=v)
rownames(df) <- NULL
wordcloud(df$word, df$freq, min.freq = 2, max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Plotly
all_vid_nerdologia %>%
mutate(proportion_like = likeCount / viewCount,
proportion_dislike = dislikeCount / viewCount) %>%
arrange(publication_date) %>%
plot_ly() %>%
add_trace(x = ~publication_date, y = ~viewCount,
name = 'View count', type = 'scatter',
mode ="markers+lines", text = ~paste('Video: ', title),
line = list(color = '#440154FF ', width = 4)) %>%
add_trace(x = ~publication_date, y = ~likeCount,
name = 'Like count', type = 'scatter',
mode ="markers+lines", text = ~paste('Video: ', title),
line = list(color = '#39568CFF', width = 4)) %>%
add_trace(x = ~publication_date, y = ~dislikeCount,
name = 'Dislike count', type = 'scatter',
mode = "markers+lines", text = ~paste('Video: ', title),
line = list(color = '#29AF7FFF', width = 4)) %>%
add_trace(x = ~publication_date, y = ~commentCount,
name = 'Comment count', type = 'scatter',
mode = "markers+lines", text = ~paste('Video: ', title),
line = list(color = '#FDE725FF', width = 4)) %>%
layout(xaxis = list(title = "Video publication date"),
yaxis = list (title = "Metric's count"),
font =  list(size = 16),
hovermode = 'compare')
all_vid_nerdologia_gather
all_video_text <- get_url_capition(video_id = all_vid_nerdologia_gather$id)
all_video_text
video_id
url_request
all_vid_nerdologia
all_video_text <- get_url_capition(video_id ='13Tos1JN_2Q')
all_video_text
all_vid_nerdologia_gather$id
all_video_text <- get_url_capition(video_id = as.character(all_vid_nerdologia_gather$id))
df_corpus
all_video_text <- list()
all_video_text <- list()
all_video_text <- list()
for(id in as.character(all_vid_nerdologia_gather$id)){
all_video_text['id'] <- get_url_capition(video_id = id))
}
as.character(all_vid_nerdologia_gather$id)
all_vid_nerdologia$id
for(id in as.character(all_vid_nerdologia$id)){
all_video_text['id'] <- get_url_capition(video_id = id))
}
all_video_text <- list()
for( id in as.character(all_vid_nerdologia$id) ){
all_video_text['id'] <- get_url_capition(video_id = id)
}
id
id
get_captions(id)
