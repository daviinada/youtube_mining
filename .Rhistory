counts
words
df_text_counts <-  cbind(words, counts)
df_text_counts
df_text_counts <- as.data.frame(df_text_counts)
str(df_text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts
df_text_counts$words <- as.character(words)
str(df_text_counts)
boxplot(df_text_counts$counts)
summary(df_text_counts)
df_text_counts %>%
filter(counts > 50)
subset(df_text_counts, counts > 10)
subset(df_text_counts, counts > 10) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
video_id <- 'ZDRHEW8fdQk'
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
page <- read_html(url_request)
text_counts <- page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts$words <- as.character(words)
str(df_text_counts)
subset(df_text_counts, counts > 50) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
subset(df_text_counts, counts > 10 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
subset(df_text_counts, counts > 20 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
# Get youtube caption
library(rvest)
library(lexiconPT)
library(ggplot2)
video_id <- 'AnkcJjzh4Bk'
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
page <- read_html(url_request)
text_counts <- page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts$words <- as.character(words)
str(df_text_counts)
boxplot(df_text_counts$counts)
summary(df_text_counts)
subset(df_text_counts, counts > 20 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
yt_oauth(client_id, key)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
yt_oauth(client_id, key)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
dim(all_vid_nerdologia)
client_id <- '812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com'
key <- 'SblVJsud6AqXdqKpQJamZRxJ'
url_request
page <- read_html(url_request)
page
html_nodes(#well)
page %>%
html_nodes('#well')
page %>%
html_nodes('#well')
page %>%
html_nodes('#well')
page %>%
html_nodes('#.well')
page %>%
html_nodes('.well')
page %>%
html_nodes('p')
page %>%
html_nodes('br')
page %>%
html_nodes('br') %>%
html_text()
page %>%
html_nodes('.well') %>%
html_text()
page %>%
html_nodes('#text')
page %>%
html_nodes('text')
page %>%
html_nodes('.text')
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
client_id <- '812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com'
key <- 'SblVJsud6AqXdqKpQJamZRxJ'
yt_oauth(client_id, key)
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
warnings()
all_vid_nerdologia
dim(all_vid_nerdologia)
saveRDS(all_vid_nerdologia, "all_vid_nerdologia.RDS")
# Get youtube caption
library(rvest)
library(lexiconPT)
library(ggplot2)
# Get youtube caption
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(dygraphs)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RColorBrewer)
setwd('/home/toshi/Desktop/work_butanta/youtube_mining/')
load('all_vid_nerdologia.RObj')
str(all_vid_nerdologia)
date_split_list <- strsplit(x = as.character(all_vid_nerdologia$publication_date), split = 'T')
date_split_list <- lapply(date_split_list, function(x){ x[1] } ) %>% unlist()
all_vid_nerdologia['publication_date'] <- date_split_list
all_vid_nerdologia$publication_date <- as.Date(all_vid_nerdologia$publication_date)
all_vid_nerdologia$viewCount <-  as.numeric(all_vid_nerdologia$viewCount)
all_vid_nerdologia$likeCount <- as.numeric(all_vid_nerdologia$likeCount)
all_vid_nerdologia$dislikeCount <- as.numeric(all_vid_nerdologia$dislikeCount)
all_vid_nerdologia$favoriteCount <- as.numeric(all_vid_nerdologia$favoriteCount)
all_vid_nerdologia$commentCount <- as.numeric(all_vid_nerdologia$commentCount)
all_vid_nerdologia[which(all_vid_nerdologia$dislikeCount == max(all_vid_nerdologia$dislikeCount)), ]
all_vid_nerdologia[which(all_vid_nerdologia$likeCount == max(all_vid_nerdologia$likeCount)), ]
all_vid_nerdologia$publication_date <- lubridate::as_datetime(all_vid_nerdologia$publication_date)
# Cleanning data title
all_vid_nerdologia$title <- gsub('\\s\\|\\s.*$', '', all_vid_nerdologia$title)
# tabela processada
all_vid_nerdologia_gather <- all_vid_nerdologia %>%
mutate(proportion_like = likeCount/ viewCount,
proportion_dislike = dislikeCount/ viewCount) %>%
gather(key= type_counts, value= counts,
c(likeCount, dislikeCount, commentCount, viewCount))
str(all_vid_nerdologia)
plot_obj <- all_vid_nerdologia_gather %>%
filter(stringr::str_detect('likeCount|dislikeCount|commentCount', type_counts)) %>%
ggplot(aes(x= publication_date, y= as.numeric(counts), col= type_counts)) +
geom_line() +
geom_point() +
scale_x_date() +
theme_bw()
str(all_vid_nerdologia)
library(RCurl)
load(html_page.RObj)
setwd('/home/toshi/Desktop/work_butanta/youtube_mining/')
load("html_page.RObj")
html_page_text <- lapply(html_page, function(x){
split_1 <- strsplit(x[[1]], '<br><br>')[[1]][2]
strsplit(split_1[[1]], '\t\t</div>')[[1]][1]
})
text_df <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_corpus <- Corpus(DataframeSource(text_df))
text_corpus_filtered <- text_corpus %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
df_corpus_tf <- TermDocumentMatrix(df_corpus_filtered, control = list(minWordLength = 4))
df_corpus_df = as.matrix(df_corpus_tf)
head(text_corpus_filtered)
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
text_corpus_df <- Corpus(DataframeSource(text_df))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_df = as.matrix(df_corpus_tf)
corpus_df = as.matrix(corpus_tf)
corpus_df <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(df_corpus_df)), decreasing=TRUE)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
corpus_m_sorted
df <- data.frame(word=names(corpus_m_sorted), freq=corpus_m_sorted)
df
df <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
df_total <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
df_total
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
save(file = 'html_page_text.RData', html_page_text)
text_df_index <- data.frame(doc_id = all_vid_nerdologia$id[2], text = unlist(html_page_text[2]), stringsAsFactors = FALSE , drop=FALSE)
text_df_index
text_corpus_df <- Corpus(DataframeSource(text_df_index))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
df_total <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(dygraphs)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RColorBrewer)
date_split_list
corpus_m_sorted
html_page_text[1]
html_page_text[2]
head(text_df_index)
text_df_index <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
head(text_df_index)
summary(text_df_index)
html_page_text[1]
html_page_text[[1]]
text_df_total <- as.data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_df_total <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_df_total
text_df_total$text =='NA'
is.na(text_df_total$text)
is.na(is.na(text_df_total$text))
which(is.na(text_df_total$text))
text_df_total[ which(is.na(text_df_total$text)), ]
text_df_total <- text_df_total[ which(is.na(text_df_total$text)), ]
knitr::opts_chunk$set(echo = TRUE)
load('../Raw_files/all_vid_nerdologia.RData')
load('../Raw_files/html_page_text.RData')
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RCurl)
client_id <- 'xxxxxxxx'
key <- 'xxxxxxxx'
yt_oauth(client_id, key)
knitr::opts_chunk$set(echo = TRUE)
all_vid_nerdologia_gather %>%
filter(str_detect('likeCount|dislikeCount|commentCount', type_counts)) %>%
ggplot(aes(x= publication_date, y= as.numeric(counts), col= type_counts)) +
geom_line(size=1, alpha=0.7) +
geom_point(size=1) +
scale_x_date() +
theme_bw() +
scale_color_manual(values = c('#440154FF', '#39568CFF', '#29AF7FFF'))
install.packages('kableExtra')
shiny::runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RCurl)
library(VennDiagram)
library(kableExtra)
load('../Raw_files/all_vid_nerdologia.RData')
load('../Raw_files/html_page.RData')
load('../Raw_files/html_page_text.RData')
# Dates
date_split_list <- strsplit(x= as.character(all_vid_nerdologia$publication_date), split= 'T')
date_split_list <- lapply(date_split_list, function(x){x[1]}) %>% unlist()
all_vid_nerdologia['publication_date'] <- date_split_list
all_vid_nerdologia$publication_date <- as.Date(all_vid_nerdologia$publication_date)
# Variable class
all_vid_nerdologia$viewCount <-  as.numeric(all_vid_nerdologia$viewCount)
all_vid_nerdologia$likeCount <- as.numeric(all_vid_nerdologia$likeCount)
all_vid_nerdologia$dislikeCount <- as.numeric(all_vid_nerdologia$dislikeCount)
all_vid_nerdologia$favoriteCount <- as.numeric(all_vid_nerdologia$favoriteCount)
all_vid_nerdologia$commentCount <- as.numeric(all_vid_nerdologia$commentCount)
# Cleanning data title
all_vid_nerdologia$title <- gsub('\\s\\|\\s.*$', '', all_vid_nerdologia$title)
saveRDS(all_vid_nerdologia, "all_vid_nerdologia.RDS")
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
save(file= 'all_vid_nerdologia.RObj', all_vid_nerdologia)
all_vid_nerdologia
save(file= 'all_vid_nerdologia.RData', all_vid_nerdologia)
runApp('~/Desktop/work_butanta/youtube_mining')
load('/home/toshi/Desktop/work_butanta/youtube_mining/Raw_files/all_vid_nerdologia.RData')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RCurl)
library(VennDiagram)
library(kableExtra)
load('all_vid_nerdologia.RData')
load('html_page.RData')
load('all_vid_nerdologia.RData')
load('../all_vid_nerdologia.RData')
load('../html_page.RData')
load('../html_page_text.RData')
# Dates
date_split_list <- strsplit(x= as.character(all_vid_nerdologia$publication_date), split= 'T')
date_split_list <- lapply(date_split_list, function(x){x[1]}) %>% unlist()
all_vid_nerdologia['publication_date'] <- date_split_list
all_vid_nerdologia$publication_date <- as.Date(all_vid_nerdologia$publication_date)
# Variable class
all_vid_nerdologia$viewCount <-  as.numeric(all_vid_nerdologia$viewCount)
all_vid_nerdologia$likeCount <- as.numeric(all_vid_nerdologia$likeCount)
all_vid_nerdologia$dislikeCount <- as.numeric(all_vid_nerdologia$dislikeCount)
all_vid_nerdologia$favoriteCount <- as.numeric(all_vid_nerdologia$favoriteCount)
all_vid_nerdologia$commentCount <- as.numeric(all_vid_nerdologia$commentCount)
# Cleanning data title
all_vid_nerdologia$title <- gsub('\\s\\|\\s.*$', '', all_vid_nerdologia$title)
all_vid_nerdologia %>%
arrange(desc(likeCount)) %>%
head(10) %>%
select(title, likeCount, dislikeCount, commentCount) %>%
kable("html") %>%
kable_styling("striped", full_width= F) %>%
column_spec(1:4, bold= T) %>%
row_spec(1:10, bold= T, color= "white", background= "#3399ff")
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',  all_vid_nerdologia$id, '&language=asr')
html_page <- list()
for(i in 1:length(url_request) ){
html_page[i] <- getURL(url_request[i])
Sys.sleep(30)
}
# Removing tags blocks
html_page_text <- lapply(html_page, function(x){
split_1 <- strsplit(x[[1]], '<br><br>')[[1]][2]
strsplit(split_1[[1]], '\t\t</div>')[[1]][1]
})
# Saving object for individual process
# save(file = 'html_page_text.RData', html_page_text)
# Generating corpus text for all avaiable data
text_df_total <- data.frame(doc_id= all_vid_nerdologia$id, text= unlist(html_page_text), stringsAsFactors= FALSE , drop=FALSE)
text_corpus_df <- Corpus(DataframeSource(text_df_total))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Creanting a term matrix
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered)
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing= TRUE)
df_total <- data.frame(word= names(corpus_m_sorted), freq= as.numeric(corpus_m_sorted))
df_total$word <- as.character(df_total$word)
df_total <- df_total[which(nchar(df_total$word) > 4), ]
df_total
df_total
html_page_text[1]
html_page_text[2]
html_page_text[2]
html_page_text[3]
len(html_page_text)
length(html_page_text)
length(html_page_text[[1]])
html_page_text[[1]]
html_page_text[[100]]
length(html_page[[1]])
html_page[[1]]
html_page[[2]]
length(html_page)
load('../all_vid_nerdologia.RData')
load('../html_page.RData')
load('../html_page_text.RData')
length(html_page)
# Removing tags blocks
html_page_text <- lapply(html_page, function(x){
split_1 <- strsplit(x[[1]], '<br><br>')[[1]][2]
strsplit(split_1[[1]], '\t\t</div>')[[1]][1]
})
# Saving object for individual process
# save(file = 'html_page_text.RData', html_page_text)
# Generating corpus text for all avaiable data
text_df_total <- data.frame(doc_id= all_vid_nerdologia$id, text= unlist(html_page_text), stringsAsFactors= FALSE , drop=FALSE)
text_corpus_df <- Corpus(DataframeSource(text_df_total))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Creanting a term matrix
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered)
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing= TRUE)
df_total <- data.frame(word= names(corpus_m_sorted), freq= as.numeric(corpus_m_sorted))
df_total$word <- as.character(df_total$word)
df_total <- df_total[which(nchar(df_total$word) > 4), ]
length(html_page_text)
names(html_page_text)
names(html_page)
names(html_page_text[2])
names(html_paget[2])
length(html_page_text)
names(html_page[2])
names(html_page[[2]])
length(html_page)
text_df_total$doc_id
df_total
df_total <- df_total[which(nchar(df_total$word) > 4), ]
runApp('~/Desktop/work_butanta/youtube_mining')
all_vid_nerdologia$title
all_vid_nerdologia$publication_date
all_vid_nerdologia %>%
mutate(proportion_like = likeCount / viewCount,
proportion_dislike = dislikeCount / viewCount) %>%
arrange(publication_date)
runApp('~/Desktop/work_butanta/youtube_mining')
all_vid_nerdologia
all_vid_nerdologia <- all_vid_nerdologia %>%
arrange(publication_date)
all_vid_nerdologia
load('all_vid_nerdologia.RData')
all_vid_nerdologia %>%
arrange(publication_date)
cursor_id <- as.numeric(eventdata$pointNumber)[1] + 1
runApp('~/Desktop/work_butanta/youtube_mining')
runApp('~/Desktop/work_butanta/youtube_mining')
