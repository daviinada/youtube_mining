scale_color_manual(values = c( '#6baed6', '#2171b5'))
poli_plot_table %>%
mutate(poli_size = end_poli - start_poli) %>%
ggplot(aes(x = poli_size, col=class)) +
geom_freqpoly(bins=50, size= 1.5) +
scale_color_manual(values = c( '#6baed6', '#2171b5'))
theme_bw() +
scale_fill_manual(values = c( '#6baed6', '#2171b5'))
poli_plot_table %>%
mutate(poli_size = end_poli - start_poli) %>%
ggplot(aes(x = poli_size, fill=class)) +
geom_histogram(bins=30, position = 'identity', alpha=0.5) +
theme_bw() +
scale_fill_manual(values = c( '#6baed6', '#2171b5'))
poli_plot_table %>%
mutate(poli_size = end_poli - start_poli) %>%
ggplot(aes(x = poli_size, fill=class)) +
geom_histogram(bins=30, position = 'identity', alpha=0.7) +
theme_bw() +
scale_fill_manual(values = c( '#6baed6', '#2171b5'))
poli_plot_table %>%
mutate(poli_size = end_poli - start_poli) %>%
ggplot(aes(x = poli_size, fill=class)) +
geom_histogram(bins=30, position = 'identity', alpha=0.7) +
theme_bw() +
scale_fill_manual(values = c('#2171b5', '#6baed6'))
install.packages("tuber")
library("tuber")
yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "MbOSt6cQhhFkwETXKur-L9rN")
library("tuber")
yt_oauth("812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com", "SblVJsud6AqXdqKpQJamZRxJ")
yt_oauth("812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com", "SblVJsud6AqXdqKpQJamZRxJ")
yt_oauth("812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com", "AIzaSyD_D_iM2M-ZGxf1AfTf_PSx3NOkq6YPfPs")
yt_oauth("812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com", "SblVJsud6AqXdqKpQJamZRxJ")
yt_oauth("812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com", "SblVJsud6AqXdqKpQJamZRxJ", token= '')
cap_felipe_neto <- get_captions(video_id="mbEoqZrquKw")
cap <- get_captions(video_id="yJXTXN4xrI8")
get_stats(video_id="N708P-A45D0")
get_stats('mbEoqZrquKw')
get_captions('mbEoqZrquKw')
get_captions(video_id="yJXTXN4xrI8")
get_stats('mbEoqZrquKw')
res <- get_comment_threads(c(video_id="mbEoqZrquKw"))
res
res$authorChannelId.value
res$authorDisplayName
res$textOriginal
res$likeCount
res$textDisplay
a <- list_channel_resources(filter = c(channel_id = "UClu474HMt895mVxZdlIHXEA"), part="contentDetails")
# Uploaded playlists:
playlist_id <- a$items[[1]]$contentDetails$relatedPlaylists$uploads
playlist_id
# Get videos on the playlist
vids <- get_playlist_items(filter= c(playlist_id=playlist_id))
vids
# Video ids
vid_ids <- as.vector(vids$contentDetails.videoId)
vid_ids
# Function to scrape stats for all vids
get_all_stats <- function(id) {
get_stats(id)
}
# Get stats and convert results to data frame
res <- lapply(vid_ids, get_all_stats)
res_df <- do.call(rbind, lapply(res, data.frame))
head(res_df)
res_df
a$items
a$items[[1]]$contentDetails$relatedPlaylists$uploads
a$items[[1]]$contentDetails$relatedPlaylists
a$items[[1]]$contentDetails
get_captions('mbEoqZrquKw')
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
video_id <- 'mbEoqZrquKw'
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
url_request
# Get youtube caption
library(rvest)
page <- read_html(url_request)
page
page %>%
html_text()
install.packages(lexiconPT)
devtools::install_github("sillasgonzaga/lexiconPT").
devtools::install_github("sillasgonzaga/lexiconPT")
page %>%
html_text()
page %>%
html_text() %>%
strsplit('\\w')
page %>%
html_text() %>%
strsplit('\\W+')
page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist()
page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table()
page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort()
page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
text_counts <- page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
names(text_counts)
words <- names(text_counts)
counts(unlist(text_counts))
text_counts
counts(as.numeric(text_counts))
text_counts
as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts
str(df_text_counts)
df_text_counts %>%
ggplot(aes(x=words, y=counts)) %>%
geom_bar()
library(ggplot2)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
str(df_text_counts)
df_text_counts %>%
ggplot(aes(x=words, y=counts)) %>%
geom_bar()
df_text_counts %>%
ggplot(aes(x=words, y=counts)) +
geom_bar()
df_text_counts %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity')
df_text_counts %>%
filter(counts < 50) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity')
df_text_counts %>%
filter(counts < 50)
df_text_counts %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity')
df_text_counts
df_text_counts %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
words <- as.character(names(text_counts))
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
str(df_text_counts)
df_text_counts$words <- as.character(words)
df_text_counts
str(df_text_counts)
df_text_counts %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
summary(df_text_counts)
df_text_counts %>%
filter(couns > 30)
df_text_counts %>%
filter(counts > 30)
str(df_text_counts)
boxplot(df_text_counts$counts)
df_text_counts %>%
filter(counts > 10)
df_text_counts %>%
filter(counts > 10) %>%
na.omit()
df_text_counts
df_text_counts %>%
filter(counts > 50)
df_text_counts <-  as.data.frame(words, counts)
df_text_counts <-  as.data.frame(words, counts)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <-  as.data.frame(words, counts)
df_text_counts <-  cbind(words, counts)
df_text_counts$words <- as.character(words)
df_text_counts
df_text_counts <-  cbind(words, counts)
words <- names(text_counts)
counts <- as.numeric(text_counts)
counts
words
df_text_counts <-  cbind(words, counts)
df_text_counts
df_text_counts <- as.data.frame(df_text_counts)
str(df_text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts
df_text_counts$words <- as.character(words)
str(df_text_counts)
boxplot(df_text_counts$counts)
summary(df_text_counts)
df_text_counts %>%
filter(counts > 50)
subset(df_text_counts, counts > 10)
subset(df_text_counts, counts > 10) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
video_id <- 'ZDRHEW8fdQk'
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
page <- read_html(url_request)
text_counts <- page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts$words <- as.character(words)
str(df_text_counts)
subset(df_text_counts, counts > 50) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
subset(df_text_counts, counts > 10 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
subset(df_text_counts, counts > 20 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
# Get youtube caption
library(rvest)
library(lexiconPT)
library(ggplot2)
video_id <- 'AnkcJjzh4Bk'
url_request <- paste0('http://diycaptions.com/php/get-automatic-captions-as-txt.php?id=',
video_id,
'&language=asr')
page <- read_html(url_request)
text_counts <- page %>%
html_text() %>%
strsplit('\\W+') %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE)
words <- names(text_counts)
counts <- as.numeric(text_counts)
df_text_counts <- data.frame(words, counts)
df_text_counts$words <- as.character(words)
str(df_text_counts)
boxplot(df_text_counts$counts)
summary(df_text_counts)
subset(df_text_counts, counts > 20 & counts < 30 ) %>%
ggplot(aes(x=words, y=counts)) +
geom_bar(stat='identity') +
coord_flip()
yt_oauth(client_id, key)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
yt_oauth(client_id, key)
get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
dim(all_vid_nerdologia)
client_id <- '812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com'
key <- 'SblVJsud6AqXdqKpQJamZRxJ'
url_request
page <- read_html(url_request)
page
html_nodes(#well)
page %>%
html_nodes('#well')
page %>%
html_nodes('#well')
page %>%
html_nodes('#well')
page %>%
html_nodes('#.well')
page %>%
html_nodes('.well')
page %>%
html_nodes('p')
page %>%
html_nodes('br')
page %>%
html_nodes('br') %>%
html_text()
page %>%
html_nodes('.well') %>%
html_text()
page %>%
html_nodes('#text')
page %>%
html_nodes('text')
page %>%
html_nodes('.text')
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
client_id <- '812757213029-tt2n55mmbm4143ehdhap21e25ihp0kse.apps.googleusercontent.com'
key <- 'SblVJsud6AqXdqKpQJamZRxJ'
yt_oauth(client_id, key)
all_vid_nerdologia <- get_all_channel_video_stats(channel_id = 'UClu474HMt895mVxZdlIHXEA', mine = FALSE)
warnings()
all_vid_nerdologia
dim(all_vid_nerdologia)
saveRDS(all_vid_nerdologia, "all_vid_nerdologia.RDS")
# Get youtube caption
library(rvest)
library(lexiconPT)
library(ggplot2)
# Get youtube caption
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(dygraphs)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RColorBrewer)
setwd('/home/toshi/Desktop/work_butanta/youtube_mining/')
load('all_vid_nerdologia.RObj')
str(all_vid_nerdologia)
date_split_list <- strsplit(x = as.character(all_vid_nerdologia$publication_date), split = 'T')
date_split_list <- lapply(date_split_list, function(x){ x[1] } ) %>% unlist()
all_vid_nerdologia['publication_date'] <- date_split_list
all_vid_nerdologia$publication_date <- as.Date(all_vid_nerdologia$publication_date)
all_vid_nerdologia$viewCount <-  as.numeric(all_vid_nerdologia$viewCount)
all_vid_nerdologia$likeCount <- as.numeric(all_vid_nerdologia$likeCount)
all_vid_nerdologia$dislikeCount <- as.numeric(all_vid_nerdologia$dislikeCount)
all_vid_nerdologia$favoriteCount <- as.numeric(all_vid_nerdologia$favoriteCount)
all_vid_nerdologia$commentCount <- as.numeric(all_vid_nerdologia$commentCount)
all_vid_nerdologia[which(all_vid_nerdologia$dislikeCount == max(all_vid_nerdologia$dislikeCount)), ]
all_vid_nerdologia[which(all_vid_nerdologia$likeCount == max(all_vid_nerdologia$likeCount)), ]
all_vid_nerdologia$publication_date <- lubridate::as_datetime(all_vid_nerdologia$publication_date)
# Cleanning data title
all_vid_nerdologia$title <- gsub('\\s\\|\\s.*$', '', all_vid_nerdologia$title)
# tabela processada
all_vid_nerdologia_gather <- all_vid_nerdologia %>%
mutate(proportion_like = likeCount/ viewCount,
proportion_dislike = dislikeCount/ viewCount) %>%
gather(key= type_counts, value= counts,
c(likeCount, dislikeCount, commentCount, viewCount))
str(all_vid_nerdologia)
plot_obj <- all_vid_nerdologia_gather %>%
filter(stringr::str_detect('likeCount|dislikeCount|commentCount', type_counts)) %>%
ggplot(aes(x= publication_date, y= as.numeric(counts), col= type_counts)) +
geom_line() +
geom_point() +
scale_x_date() +
theme_bw()
str(all_vid_nerdologia)
library(RCurl)
load(html_page.RObj)
setwd('/home/toshi/Desktop/work_butanta/youtube_mining/')
load("html_page.RObj")
html_page_text <- lapply(html_page, function(x){
split_1 <- strsplit(x[[1]], '<br><br>')[[1]][2]
strsplit(split_1[[1]], '\t\t</div>')[[1]][1]
})
text_df <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_corpus <- Corpus(DataframeSource(text_df))
text_corpus_filtered <- text_corpus %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
df_corpus_tf <- TermDocumentMatrix(df_corpus_filtered, control = list(minWordLength = 4))
df_corpus_df = as.matrix(df_corpus_tf)
head(text_corpus_filtered)
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
text_corpus_df <- Corpus(DataframeSource(text_df))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_df = as.matrix(df_corpus_tf)
corpus_df = as.matrix(corpus_tf)
corpus_df <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(df_corpus_df)), decreasing=TRUE)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
corpus_m_sorted
df <- data.frame(word=names(corpus_m_sorted), freq=corpus_m_sorted)
df
df <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
df_total <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
df_total
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
save(file = 'html_page_text.RData', html_page_text)
text_df_index <- data.frame(doc_id = all_vid_nerdologia$id[2], text = unlist(html_page_text[2]), stringsAsFactors = FALSE , drop=FALSE)
text_df_index
text_corpus_df <- Corpus(DataframeSource(text_df_index))
text_corpus_df_filtered <- text_corpus_df %>%
tm_map(stripWhitespace)  %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers)   %>%
tm_map(removeWords, c(stopwords("portuguese"))) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(content_transformer(tolower))
# Criando a matrix de termos:
corpus_tf <- TermDocumentMatrix(text_corpus_df_filtered, control = list(minWordLength = 3))
corpus_m <- as.matrix(corpus_tf)
corpus_m_sorted <- sort(rowSums(as.matrix(corpus_m)), decreasing=TRUE)
df_total <- data.frame(word=names(corpus_m_sorted), freq=as.numeric(corpus_m_sorted))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(df_total$word, df_total$freq, min.freq = 3, max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(dygraphs)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RColorBrewer)
date_split_list
corpus_m_sorted
html_page_text[1]
html_page_text[2]
head(text_df_index)
text_df_index <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
head(text_df_index)
summary(text_df_index)
html_page_text[1]
html_page_text[[1]]
text_df_total <- as.data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_df_total <- data.frame(doc_id = all_vid_nerdologia$id, text = unlist(html_page_text), stringsAsFactors = FALSE , drop=FALSE)
text_df_total
text_df_total$text =='NA'
is.na(text_df_total$text)
is.na(is.na(text_df_total$text))
which(is.na(text_df_total$text))
text_df_total[ which(is.na(text_df_total$text)), ]
text_df_total <- text_df_total[ which(is.na(text_df_total$text)), ]
knitr::opts_chunk$set(echo = TRUE)
load('../Raw_files/all_vid_nerdologia.RData')
load('../Raw_files/html_page_text.RData')
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(plotly)
library(tuber)
library(stringr)
library(tm)
library(wordcloud)
library(RCurl)
client_id <- 'xxxxxxxx'
key <- 'xxxxxxxx'
yt_oauth(client_id, key)
